{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三引号遵从所见即所得，允许字符串换行显示，可加制表符、换行符等多样化显示效果\n",
    "import random\n",
    "hello_rules = '''    \n",
    "say_hello = names hello tail\n",
    "names = name names | name\n",
    "name = Jhon | Mike | 老梁 | 老刘\n",
    "hello = 你好 | 您来啦 | 快请进\n",
    "tail = 呀 | ！\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'add = number + number'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'add = number + number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generation_by_gram(grammer_str: str, target, stmt_split = '=', or_split = '|'):\n",
    "    rules = dict()\n",
    "    for line in grammer_str.split('\\n'):\n",
    "        if not line: continue\n",
    "        \n",
    "        stmt, expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = expr.split(or_split) #=split分割字符串，将各分割字符串以列表形式保存\n",
    "    generated = generate(rules,target=target) #默认target的值\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammer_rule, target):\n",
    "    if target in grammer_rule:\n",
    "        candidates = grammer_rule[target]\n",
    "\n",
    "        candidate = random.choice(candidates)\n",
    "\n",
    "        return ''.join(generate(grammer_rule,target=c.strip()) for c in candidate.split())\n",
    "        \n",
    "    else:\n",
    "            return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_grammer = '''\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* =>Adj | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article => 一个 | 这个\n",
    "noun => 女人 | 篮球 | 桌子 | 小猫\n",
    "verb => 看着 | 坐着 | 听着 | 看见\n",
    "Adj => 蓝色的 | 好看的 | 小小的\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这个蓝色的小猫坐着一个蓝色的好看的好看的桌子'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_generation_by_gram(simple_grammer, target= 'sentence', stmt_split = '=>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_programming = '''\n",
    "if_stmt => if ( cond ) { stmt }\n",
    "cond => var op var\n",
    "op => | == | < | >= | <=\n",
    "stmt => assign | if_stmt\n",
    "assign => var = var\n",
    "var => char var | char\n",
    "char => a | b | c | d | 0 | 1 | 2 | 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if(d==3){if(b==aa3){if(0<=bdcc){if(bda==22){2a=b3}}}}\n",
      "if(0>=aa){if(cd>=d){if(c1>=0){if(c<=bc){if(cd==2){if(c==b){if(2==c){0=2d}}}}}}}\n",
      "if(b<21){if(c1){aba=da}}\n",
      "if(c12<1){if(102c<=c2){if(0ddd<0){if(20c<1){if(cd0){1ccc=b}}}}}\n",
      "if(1<b0a){ac=2}\n",
      "if(d210dd>=112){cd=c13b1bb}\n",
      "if(12<ac11){32=22}\n",
      "if(3320<=3c){if(aa2){c3=21033}}\n",
      "if(d0){if(30333){if(1>=d){b=dcc}}}\n",
      "if(3c<=d){3a=3}\n",
      "if(d<=10){if(c20c02){if(030a<d){b=3}}}\n",
      "if(b<=0d){dd=b}\n",
      "if(d121>=c3){3b=acc3}\n",
      "if(dd<c){if(b==01cab31){3=1}}\n",
      "if(2<21){if(cc==a){if(c32<=3){if(1<=a){if(22bdc0<d){if(1b<a20){0=c}}}}}}\n",
      "if(3==c1){if(c3<=ac){ad=ac2}}\n",
      "if(23ab){if(1<c){if(02==3){if(2>=1dbcc){if(02<=03c){0=1}}}}}\n",
      "if(db<1da){d=b}\n",
      "if(2c==b){if(1>=1c){b3=abab}}\n",
      "if(a1==c){b02=cc}\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(get_generation_by_gram(simpel_programming, 'if_stmt',stmt_split = '=>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name():\n",
    "    return random.choice('Jhon | Mike | 老梁'.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello():\n",
    "    return random.choice('你好 | 您来啦 | 快请进'.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hello():\n",
    "    return name() + ' '+ hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'D:/study_python1/datesource/article_9k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/study_python1/datesource/article_9k.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7ccb153c5f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mFILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#打开方式用UTF-8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/study_python1/datesource/article_9k.txt'"
     ]
    }
   ],
   "source": [
    "FILE = open(corpus,'r', encoding='UTF-8').read() #打开方式用UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_by_pro(text_corpus,length=20):\n",
    "    return ''.join(random.sample(text_corpus, length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-19cc71952a89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'FILE' is not defined"
     ]
    }
   ],
   "source": [
    "len(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000000\n",
    "sub_file = FILE[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = cut(sub_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_fre  =[f for w,f in words_count.most_common()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_fre[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(words_with_fre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(jieba.cut('一加手机5要做市面最轻薄'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3_gram_words = [TOKENS[i]+TOKENS[i+1]+TOKENS[i+2] for i in range(len(TOKENS)-1) if i != len(TOKENS)-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3_gram_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3_gram_words_counts = Counter(_3_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1_gram_count():\n",
    "    if word in words_count:\n",
    "        return word_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3_gram_count(word):\n",
    "    if word in _3_gram_words_counts:\n",
    "        return _3_gram_words_counts[word]\n",
    "    else:\n",
    "        return _3_gram_word_counts.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_count(word,wc):\n",
    "    if word in wc:return wc[word]\n",
    "    else:return wc.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gram_count(\"XXX\",words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gram_count(\"手机\",_3_gram_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    \n",
    "    probability = 1\n",
    "    for i in range(len(tokens)-2):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        next_2_word = tokens[i+2]\n",
    "        three_gram_c = get_gram_count(word + next_word+next_2_word,_3_gram_words_counts)\n",
    "        print(three_gram_c)\n",
    "        one_gram_c = get_gram_count(next_word,words_count)\n",
    "        probability *= three_gram_c/one_gram_c\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_gram_model('前天早上吃晚饭的时候')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. 尝试了，貌似正确，但是结果不对"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用jupyter和pycharm方便操作，可以逐行排查，可视化效果好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 事件的发生概率依赖于其他事件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 判断语境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 推断大概率发生的事件，接近实际；难点在于词语之间的依赖关系复杂，不仅限于紧邻词语，且词语包含多种含义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 语言模型：根据判断序列的概率推测为句子的可能性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 机器翻译、语音识别等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 词语之间单独存在不依赖于前后的词语的序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 缺点：准确率不高，不符合实际情况；优点：便于计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 每一个词出现的的概率依赖于之前的一个词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计自己的句子生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "you_need_replace_this_with_name_you_given = '''\n",
    "I = name 进行 事情 \n",
    "name = Lousic | Jack | 阿标\n",
    "进行 = 干 | 在\n",
    "事情 = 逛街 | 工作 | 学习 | 玩耍\n",
    "状态 = 周内 | 星期天\n",
    "时间 = 上午 | 中午 | 下午\n",
    "工作 = 状态 时间 紧急 ！\n",
    "紧急 = 紧急 | 不急\n",
    "学习 = 工作相关 | 生活相关\n",
    "玩耍 = 户外 | 室内\n",
    "结尾 = 开心呀！\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generate(input_string, target, first_split = '=', second_split = '|'):\n",
    "    rule = dict()\n",
    "    for line in input_string.split('\\n'):\n",
    "        if not line: continue\n",
    "            \n",
    "        first, second= line.split(first_split)#split产生列表，为什么可以赋值给两个变量\n",
    "\n",
    "        rule[first.strip()] = second.split(second_split)\n",
    "        #value = [c.strip() for c in second.split(second_split)]\n",
    "        #rule[first] = value\n",
    "    \n",
    "    generated = generate(rule, target) #从字典rule中遍历迭代\n",
    "    \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate(rule,target): #启动的入口就是设置一个目标值从目标值开始找\n",
    "    \n",
    "    if target in rule:\n",
    "        cadidates = rule[target]\n",
    "        cadidate = random.choice(cadidates) # choice需要代入变量名字，choice为random的方法\n",
    "        \n",
    "        return ''.join(generate(rule,c.strip()) for c in cadidate.split())\n",
    "\n",
    "    else:\n",
    "        return target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'阿标在生活相关'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_generate(you_need_replace_this_with_name_you_given,\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(rule, target, n):\n",
    "    sentence = []\n",
    "    for i in range(n):\n",
    "        sentence.append(get_generate(rule, target))\n",
    "    return sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack在生活相关', 'Lousic在逛街', 'Jack在室内', 'Jack在逛街']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(you_need_replace_this_with_name_you_given,\"I\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#address = 'D:/study_python1/datesource/moive_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'E:/fgit/NLP_lesson/01/moive_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吴京意淫到了脑残的地步，看了恶心想吐', '首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹', '吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋律，为了煽情而煽情，让人觉得他是个大做作、大谎言家。（7.29更新）片子整体不如湄公河行动，1.整体不够流畅，编剧有毒，台词尴尬；2.刻意做作的主旋律煽情显得如此不合时宜而又多余。', '凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。', '中二得很', '“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。', '脑子是个好东西，希望编剧们都能有。', '三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心，他终于抛弃李忠志了，新增外来班底让硬件实力有机会和国际接轨，开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶，在理念上，它甚至做到《绣春刀2》最想做到的那部分。', '开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹簧床架挡炸弹 空手接碎玻璃 弹匣割喉等帅得飞起！就算前半段铺垫节奏散漫主角光环开太大等也不怕 作为一个中国人 两个小时弥漫着中国强大得不可侵犯的氛围 还是让那颗民族自豪心砰砰砰跳个不停。', '15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个人，无能的政府需要求助于这些英雄才能解决难题，体现的是个人的价值，所以主旋律照抄这种模式实际上是有问题的。我们以前嘲笑个人英雄主义，却没想到捆绑爱国主义的全能战士更加难以下咽。']\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(address, encoding='UTF-8') #先打开文件，然后再读\n",
    "file.head()\n",
    "articles = file['comment'].tolist()\n",
    "print(articles[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall('\\w+',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_str = str(articles[:10]) #cut必须是字符串，所以从列表读出来以后要转化成str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_word = jieba.cut(article_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.883 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "counter_cut_article = Counter(article_word) #分词后看每个词语出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[': 1,\n",
       "         \"'\": 20,\n",
       "         '吴京': 7,\n",
       "         '意淫': 2,\n",
       "         '到': 1,\n",
       "         '了': 6,\n",
       "         '脑残': 1,\n",
       "         '的': 21,\n",
       "         '地步': 1,\n",
       "         '，': 32,\n",
       "         '看': 3,\n",
       "         '恶心': 1,\n",
       "         '想': 1,\n",
       "         '吐': 1,\n",
       "         ',': 9,\n",
       "         ' ': 17,\n",
       "         '首映礼': 1,\n",
       "         '。': 12,\n",
       "         '太': 1,\n",
       "         '恐怖': 1,\n",
       "         '这个': 2,\n",
       "         '电影': 2,\n",
       "         '不讲道理': 1,\n",
       "         '完全': 1,\n",
       "         '就是': 1,\n",
       "         '在': 4,\n",
       "         '实现': 1,\n",
       "         '他': 3,\n",
       "         '小': 1,\n",
       "         '粉红': 1,\n",
       "         '英雄': 2,\n",
       "         '梦': 1,\n",
       "         '各种': 2,\n",
       "         '装备': 1,\n",
       "         '轮番': 1,\n",
       "         '上场': 1,\n",
       "         '视': 1,\n",
       "         '物理': 1,\n",
       "         '逻辑': 1,\n",
       "         '于': 1,\n",
       "         '不顾': 1,\n",
       "         '不得不': 1,\n",
       "         '说': 1,\n",
       "         '有钱': 1,\n",
       "         '真': 1,\n",
       "         '好': 3,\n",
       "         '随意': 1,\n",
       "         '胡闹': 1,\n",
       "         '炒作': 2,\n",
       "         '水平': 2,\n",
       "         '不输': 1,\n",
       "         '冯小刚': 1,\n",
       "         '但小刚': 1,\n",
       "         '至少': 1,\n",
       "         '不会': 1,\n",
       "         '用': 1,\n",
       "         '主旋律': 6,\n",
       "         '来': 1,\n",
       "         '…': 1,\n",
       "         '让': 6,\n",
       "         '人': 4,\n",
       "         '不': 1,\n",
       "         '舒服': 1,\n",
       "         '为了': 2,\n",
       "         '而': 3,\n",
       "         '煽情': 3,\n",
       "         '觉得': 1,\n",
       "         '是': 4,\n",
       "         '个': 2,\n",
       "         '大': 2,\n",
       "         '做作': 2,\n",
       "         '、': 1,\n",
       "         '谎言': 1,\n",
       "         '家': 1,\n",
       "         '（': 1,\n",
       "         '7.29': 1,\n",
       "         '更新': 1,\n",
       "         '）': 1,\n",
       "         '片子': 1,\n",
       "         '整体': 2,\n",
       "         '不如': 1,\n",
       "         '湄公河': 2,\n",
       "         '行动': 2,\n",
       "         '1': 2,\n",
       "         '.': 2,\n",
       "         '不够': 1,\n",
       "         '流畅': 1,\n",
       "         '编剧': 2,\n",
       "         '有毒': 1,\n",
       "         '台词': 1,\n",
       "         '尴尬': 1,\n",
       "         '；': 1,\n",
       "         '2': 2,\n",
       "         '刻意': 1,\n",
       "         '显得': 1,\n",
       "         '如此': 1,\n",
       "         '不合时宜': 1,\n",
       "         '又': 2,\n",
       "         '多余': 1,\n",
       "         '凭良心说': 1,\n",
       "         '看到': 1,\n",
       "         '不像': 1,\n",
       "         '《': 3,\n",
       "         '战狼': 1,\n",
       "         '》': 3,\n",
       "         '续集': 1,\n",
       "         '完虐': 1,\n",
       "         '中二得': 1,\n",
       "         '很': 1,\n",
       "         '“': 1,\n",
       "         '犯': 1,\n",
       "         '我': 1,\n",
       "         '中华': 1,\n",
       "         '者': 1,\n",
       "         '虽远必': 1,\n",
       "         '诛': 1,\n",
       "         '”': 1,\n",
       "         '比': 1,\n",
       "         '这句': 1,\n",
       "         '话': 1,\n",
       "         '还要': 1,\n",
       "         '一百倍': 1,\n",
       "         '脑子': 1,\n",
       "         '东西': 1,\n",
       "         '希望': 1,\n",
       "         '们': 1,\n",
       "         '都': 2,\n",
       "         '能': 1,\n",
       "         '有': 3,\n",
       "         '三星': 1,\n",
       "         '半': 1,\n",
       "         '实打实': 2,\n",
       "         '7': 1,\n",
       "         '分': 1,\n",
       "         '第一集': 1,\n",
       "         '爱国': 1,\n",
       "         '内部': 1,\n",
       "         '做': 1,\n",
       "         '着': 1,\n",
       "         '置换': 1,\n",
       "         '与': 1,\n",
       "         '较劲': 1,\n",
       "         '但': 2,\n",
       "         '第二集': 1,\n",
       "         '才': 1,\n",
       "         '真正': 1,\n",
       "         '显露': 1,\n",
       "         '野心': 1,\n",
       "         '终于': 1,\n",
       "         '抛弃': 1,\n",
       "         '李忠志': 1,\n",
       "         '新增': 1,\n",
       "         '外来': 1,\n",
       "         '班底': 1,\n",
       "         '硬件': 1,\n",
       "         '实力': 1,\n",
       "         '机会': 1,\n",
       "         '和': 2,\n",
       "         '国际': 1,\n",
       "         '接轨': 1,\n",
       "         '开篇': 2,\n",
       "         '水下': 1,\n",
       "         '长镜头': 2,\n",
       "         '诸如': 1,\n",
       "         '铁丝网': 1,\n",
       "         '拦截': 1,\n",
       "         'RPG': 1,\n",
       "         '弹头': 1,\n",
       "         '细节': 1,\n",
       "         '设计': 1,\n",
       "         '国产': 1,\n",
       "         '动作片': 1,\n",
       "         '重新': 1,\n",
       "         '封顶': 1,\n",
       "         '理念': 1,\n",
       "         '上': 1,\n",
       "         '它': 1,\n",
       "         '甚至': 1,\n",
       "         '做到': 1,\n",
       "         '绣春刀': 1,\n",
       "         '最': 1,\n",
       "         '想做到': 1,\n",
       "         '那': 1,\n",
       "         '部分': 1,\n",
       "         '惊险': 1,\n",
       "         '大气': 1,\n",
       "         '引人入胜': 1,\n",
       "         '结合': 1,\n",
       "         '不俗': 1,\n",
       "         '快': 1,\n",
       "         '剪下': 1,\n",
       "         '真刀真枪': 1,\n",
       "         '不禁': 1,\n",
       "         '热血沸腾': 1,\n",
       "         '特别': 1,\n",
       "         '弹簧床': 1,\n",
       "         '架': 1,\n",
       "         '挡': 1,\n",
       "         '炸弹': 1,\n",
       "         '空手': 1,\n",
       "         '接': 1,\n",
       "         '碎玻璃': 1,\n",
       "         '弹匣': 1,\n",
       "         '割喉': 1,\n",
       "         '等': 2,\n",
       "         '帅': 1,\n",
       "         '得': 2,\n",
       "         '飞起': 1,\n",
       "         '！': 1,\n",
       "         '就算': 1,\n",
       "         '前半段': 1,\n",
       "         '铺垫': 1,\n",
       "         '节奏': 1,\n",
       "         '散漫': 1,\n",
       "         '主角': 2,\n",
       "         '光环': 1,\n",
       "         '开太大': 1,\n",
       "         '也': 1,\n",
       "         '不怕': 1,\n",
       "         '作为': 1,\n",
       "         '一个': 1,\n",
       "         '中国': 2,\n",
       "         '两个': 1,\n",
       "         '小时': 1,\n",
       "         '弥漫着': 1,\n",
       "         '强大': 1,\n",
       "         '不可': 1,\n",
       "         '侵犯': 1,\n",
       "         '氛围': 1,\n",
       "         '还是': 1,\n",
       "         '那颗': 1,\n",
       "         '民族': 1,\n",
       "         '自豪': 1,\n",
       "         '心': 1,\n",
       "         '砰砰': 1,\n",
       "         '砰': 1,\n",
       "         '跳个': 1,\n",
       "         '不停': 1,\n",
       "         '15': 1,\n",
       "         '/': 1,\n",
       "         '100': 1,\n",
       "         '冷峰': 1,\n",
       "         '这部': 1,\n",
       "         '里': 1,\n",
       "         '即': 1,\n",
       "         '像': 1,\n",
       "         '成龙': 1,\n",
       "         '像杰': 1,\n",
       "         '森斯坦': 1,\n",
       "         '森': 1,\n",
       "         '体制': 1,\n",
       "         '外': 1,\n",
       "         '同': 1,\n",
       "         '类型': 1,\n",
       "         '总是': 1,\n",
       "         '代表': 1,\n",
       "         '个人': 3,\n",
       "         '无能': 1,\n",
       "         '政府': 1,\n",
       "         '需要': 1,\n",
       "         '求助于': 1,\n",
       "         '这些': 1,\n",
       "         '才能': 1,\n",
       "         '解决': 1,\n",
       "         '难题': 1,\n",
       "         '体现': 1,\n",
       "         '价值': 1,\n",
       "         '所以': 1,\n",
       "         '照抄': 1,\n",
       "         '这种': 1,\n",
       "         '模式': 1,\n",
       "         '实际上': 1,\n",
       "         '问题': 1,\n",
       "         '我们': 1,\n",
       "         '以前': 1,\n",
       "         '嘲笑': 1,\n",
       "         '英雄主义': 1,\n",
       "         '却': 1,\n",
       "         '没想到': 1,\n",
       "         '捆绑': 1,\n",
       "         '爱国主义': 1,\n",
       "         '全能': 1,\n",
       "         '战士': 1,\n",
       "         '更加': 1,\n",
       "         '难以': 1,\n",
       "         '下咽': 1,\n",
       "         ']': 1})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_cut_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('，', 32),\n",
       " ('的', 21),\n",
       " (\"'\", 20),\n",
       " (' ', 17),\n",
       " ('。', 12),\n",
       " (',', 9),\n",
       " ('吴京', 7),\n",
       " ('了', 6),\n",
       " ('主旋律', 6),\n",
       " ('让', 6)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_cut_article.most_common()[:10] #看排名前几的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴京意淫到了脑残的地步看了恶心想吐首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家729更新片子整体不如湄公河行动1整体不够流畅编剧有毒台词尴尬2刻意做作的主旋律煽情显得如此不合时宜而又多余凭良心说好看到不像战狼1的续集完虐湄公河行动中二得很犯我中华者虽远必诛吴京比这句话还要意淫一百倍脑子是个好东西希望编剧们都能有三星半实打实的7分第一集在爱国主旋律内部做着各种置换与较劲但第二集才真正显露吴京的野心他终于抛弃李忠志了新增外来班底让硬件实力有机会和国际接轨开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶在理念上它甚至做到绣春刀2最想做到的那部分开篇长镜头惊险大气引人入胜结合了水平不俗的快剪下实打实的真刀真枪让人不禁热血沸腾特别弹簧床架挡炸弹空手接碎玻璃弹匣割喉等帅得飞起就算前半段铺垫节奏散漫主角光环开太大等也不怕作为一个中国人两个小时弥漫着中国强大得不可侵犯的氛围还是让那颗民族自豪心砰砰砰跳个不停15100吴京的冷峰在这部里即像成龙又像杰森斯坦森但体制外的同类型电影主角总是代表个人无能的政府需要求助于这些英雄才能解决难题体现的是个人的价值所以主旋律照抄这种模式实际上是有问题的我们以前嘲笑个人英雄主义却没想到捆绑爱国主义的全能战士更加难以下咽'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token(article_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_clean = [''.join(token(str(a))) for a in articles] #?为什么a要转化为str才可以？a以什么为节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('movie_article.txt','w',encoding='utf-8') as f: #打不开加encoding\n",
    "with open('movie_article.txt','w',encoding='utf-8') as f:\n",
    "    for a in token_clean:\n",
    "        f.write(a +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "TOKEN = []\n",
    "for i, line in enumerate(open('movie_article.txt','r',encoding='utf-8')):\n",
    "    if i % 100 == 0: print(i) #作用是什么？\n",
    "    if i>10000: break\n",
    "    TOKEN += jieba.cut(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 16095),\n",
       " ('\\n', 10001),\n",
       " ('了', 6113),\n",
       " ('是', 3932),\n",
       " ('我', 2677),\n",
       " ('都', 2243),\n",
       " ('和', 1837),\n",
       " ('也', 1741),\n",
       " ('在', 1718),\n",
       " ('看', 1603)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = [str(t) for t in TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_1(word):\n",
    "    return counter[word]/len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN2 = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('了\\n', 971),\n",
       " ('啊\\n', 353),\n",
       " ('的\\n', 350),\n",
       " ('都是', 341),\n",
       " ('的电影', 340),\n",
       " ('让人', 334),\n",
       " ('看的', 291),\n",
       " ('也是', 250),\n",
       " ('\\n我', 247),\n",
       " ('的是', 243)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_2 = Counter(TOKEN2)\n",
    "counter_2.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_2(word1,word2): #?为什么要传入两个word？不转化str是不是因为函数默认传入str\n",
    "    #word2word = str(word1) + str(word2)\n",
    "\n",
    "    if word1 + word2 in counter_2: return counter_2[word1 + word2]/len(TOKEN2)#counter是统计词语出现次数的词典\n",
    "    else:return 1/len(TOKEN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013035218024533733"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_2('都','是')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_pro(sentence):\n",
    "    sentence_cut = list(jieba.cut(sentence)) #需转化成list才可以输出\n",
    "    probability = 1\n",
    "    for i, word in enumerate(sentence_cut[:-1]):\n",
    "        next_word = sentence_cut[i+1]\n",
    "        \n",
    "        probability*=pro_2(word,next_word)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.007550314897578e-25\n",
      "4.653039752783364e-12\n"
     ]
    }
   ],
   "source": [
    "print(sentence_pro('战狼这个电影真不好吃'))\n",
    "print(sentence_pro('吴京的电影好看'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''\n",
    "吴京的电影好看吗，我觉得好看\n",
    "战狼是个爱国大片，紧跟时代的步伐\n",
    "战狼有点夸张，不好看\n",
    "战狼这个电影是个爱国题材的，情节紧凑，但是部分不连贯\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(input_string, first_split = '=', second_split = '|'):\n",
    "    rule = dict()\n",
    "    for line in input_string.split('\\n'):\n",
    "        if not line: continue\n",
    "            \n",
    "        first, second= line.split(first_split)#split产生列表，为什么可以赋值给两个变量\n",
    "\n",
    "        rule[first.strip()] = [second.split(second_split)]\n",
    "    return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': [[' name 进行 事情 ']],\n",
       " 'name': [[' Lousic ', ' Jack ', ' 阿标']],\n",
       " '进行': [[' 干 ', ' 在']],\n",
       " '事情': [[' 逛街 ', ' 工作 ', ' 学习 ', ' 玩耍']],\n",
       " '状态': [[' 周内 ', ' 星期天']],\n",
       " '时间': [[' 上午 ', ' 中午 ', ' 下午']],\n",
       " '工作': [[' 状态 时间 紧急 ！']],\n",
       " '紧急': [[' 紧急 ', ' 不急']],\n",
       " '学习': [[' 工作相关 ', ' 生活相关']],\n",
       " '玩耍': [[' 户外 ', ' 室内']],\n",
       " '结尾': [[' 开心呀！']]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = get_dict(you_need_replace_this_with_name_you_given)\n",
    "random_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Lousic干生活相关',\n",
       "  '阿标在周内下午不急！',\n",
       "  '阿标干逛街',\n",
       "  'Jack干户外',\n",
       "  'Jack干室内',\n",
       "  'Jack在逛街',\n",
       "  'Jack在室内',\n",
       "  '阿标干户外',\n",
       "  '阿标干户外',\n",
       "  'Jack在生活相关']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[generate_n(you_need_replace_this_with_name_you_given,\"I\", 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_1 = ['{},{}'.format(sen,sentence_pro(sen)) for sen in generate_n(you_need_replace_this_with_name_you_given,\"I\", 10)]  #return和def匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jack在室内,1.4612611600100538e-11',\n",
       " 'Lousic干逛街,1.4612611600100538e-11',\n",
       " '阿标干室内,1.4612611600100538e-11',\n",
       " '阿标干周内中午不急！,3.12020783453582e-33',\n",
       " 'Jack在周内中午不急！,3.12020783453582e-33',\n",
       " 'Jack在周内下午不急！,3.12020783453582e-33',\n",
       " 'Lousic在生活相关,5.5858820561625e-17',\n",
       " 'Lousic干工作相关,5.5858820561625e-17',\n",
       " '阿标在逛街,1.4612611600100538e-11',\n",
       " 'Lousic在户外,1.4612611600100538e-11']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_pro = {}\n",
    "def sentence_pro(list_1):\n",
    "    for line in list_1:\n",
    "        first, second = line.split(',') \n",
    "        sen_pro[first]= second\n",
    "    return sen_pro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('阿标干周内中午不急！', 3.12020783453582e-33), ('Jack在周内中午不急！', 3.12020783453582e-33), ('Jack在周内下午不急！', 3.12020783453582e-33), ('Lousic在生活相关', 5.5858820561625e-17), ('Lousic干工作相关', 5.5858820561625e-17), ('Jack在室内', 1.4612611600100538e-11), ('Lousic干逛街', 1.4612611600100538e-11), ('阿标干室内', 1.4612611600100538e-11), ('阿标在逛街', 1.4612611600100538e-11), ('Lousic在户外', 1.4612611600100538e-11)]\n"
     ]
    }
   ],
   "source": [
    "list_sort = []\n",
    "for i in list_1:\n",
    "    first,second = i.split(',')\n",
    "    second = float(second)\n",
    "    list_sort.append((first, second))\n",
    "array = sorted(list_sort,key = lambda x:x[1])\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
